# -*- coding: utf-8 -*-
"""test.py

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/12no2GEjZoncVl4znIogXgs9oG9udzjYg
"""

import json
import math
import string
import re
import numpy as np
from random import sample
import torch
from torch.utils.data import Dataset, DataLoader
import copy
import torch.nn as nn
import torch.nn.functional as F
from tqdm import tqdm
from scipy.special import expit
import sys

np.random.seed(42)

def load_jsonl(file_path):
    data = []
    with open(file_path, 'r') as file:
        for line in file:
            data.append(json.loads(line))
    return data


Valid_jsonl_data = load_jsonl(sys.argv[1])
Valid_data_len = len(Valid_jsonl_data)

def list_to_store_questions_table_validation(data):

    dict_to_store_questions_table = []
    for i in range(len(data)):
        dict = {}
        questions = data[i]["question"]
        dict["Question"] = questions
        dict["Columns"] = data[i]["table"]["cols"]
        dict["Row"] = data[i]["table"]["rows"]
        dict["qid"] = data[i]["qid"]
        dict_to_store_questions_table.append(dict)
    return dict_to_store_questions_table


Validation_dict_to_store_questions_table = list_to_store_questions_table_validation(Valid_jsonl_data)



import zipfile
import os

def Unzipping_the_data(Zip_path,Output):
    with zipfile.ZipFile(Zip_path, 'r') as fp:
        fp.extractall(Output)

Unzipping_the_data('2019MT60763.zip' , '2019MT60763')


with open('2019MT60763/2019MT60763/vocab.json','r') as f:
    vocab=json.load(f)
with open('2019MT60763/2019MT60763/reverse_vocab.json','r') as f:
    reverse_building_vocablary=json.load(f)



import gensim.downloader as api
glove_embeddings = api.load("glove-wiki-gigaword-100")

def finding_embedding_of_word(word,glove_embedding):
    if(word in glove_embedding):
        return glove_embedding[word]
    else:
        return np.random.rand(100)

def create_embedding_matrix(word_index, embeddings_index):
    embedding_matrix = np.zeros((len(word_index) + 1, 100))
    for word, i in word_index.items():
        word_list = word.split()
        if(len(word_list)>1):
            p = []
            for i in range(len(word_list)):
                p.append(finding_embedding_of_word(word_list[i],embeddings_index))
            t = np.mean(np.stack(p), axis=0)
            embedding_matrix[i] = t
        else:

            if(word in embeddings_index):
                embedding_matrix[i] = embeddings_index[word]
            else:
                embedding_matrix[i] = np.random.rand(100)
    return embedding_matrix

embedding_matrix = create_embedding_matrix(vocab, glove_embeddings)


def replacing_with_id_validation(dict_to_store_questions_table,vocab):

    list_to_store_questions_table_id = []
    for i in range(len(dict_to_store_questions_table)):
        dict = {}
        L = dict_to_store_questions_table[i]
        Question = L["Question"]
        Question_list = Question.split()
        p = []
        for j in range(len(Question_list)):
            if(Question_list[j] in vocab):
                p.append(vocab[Question_list[j]])
            else:
                p.append(vocab["<unk>"])
        dict["Question"] = p
        t = []
        Columns = L["Columns"]
        for j in range(len(Columns)):
            if(Columns[j] in vocab):
                t.append(vocab[Columns[j]])
            else:
                t.append(vocab["<unk>"])
        dict["Columns"] = t
        temp = []
        Rows = L["Row"]
        for j in range(len(Rows)):
            E = Rows[j]
            r=[]
            for k in range(len(E)):
                if(E[k] in vocab):
                    r.append(vocab[E[k]])
                else:
                    r.append(vocab["<unk>"])
            temp.append(r)
        dict["Rows"]=temp
        dict["qid"] = L["qid"]
        list_to_store_questions_table_id.append(dict)
    return list_to_store_questions_table_id

validation_List_for_storing_question_and_column_with_id = replacing_with_id_validation(Validation_dict_to_store_questions_table,vocab)

def preparing_input_data(List_for_storing_question_and_column_with_id):
    Questions = []
    Columns = []
    Rows = []
    Rows_length = []
    Qid = []
    for i in range(len(List_for_storing_question_and_column_with_id)):
        L = List_for_storing_question_and_column_with_id[i]
        Questions.append(L["Question"])
        Columns.append(L["Columns"])
        Rows.append(L["Rows"])
        Rows_length.append(len(L["Rows"]))
        Qid.append(L["qid"])
    return Questions,Columns,Rows,Rows_length,Qid

validaton_data_Questions,validation_data_Columns,validation_data_Rows,validation_data_Rows_length,validation_Qid = preparing_input_data(validation_List_for_storing_question_and_column_with_id)

validation_data = []

for i in range(len(validaton_data_Questions)):
    validation_data.append((validaton_data_Questions[i],validation_data_Columns[i]))

validation_data_row = []

for i in range(len(validation_data_Rows)):
    for j in range(len(validation_data_Rows[i])):
          validation_data_row.append((validaton_data_Questions[i],validation_data_Rows[i][j]))

def collate_validation(batch):
    sentences = [x[0] for x in batch]
    max_len = max(len(s) for s in sentences)
    s = torch.zeros((len(sentences), max_len), dtype=torch.long)
    for i, sentence in enumerate(sentences):
        s[i, :len(sentence)] = torch.LongTensor(sentence)

    tags = [x[1] for x in batch]
    max_tag_len = max(len(t) for t in tags)
    t = torch.zeros((len(tags), max_tag_len), dtype=torch.long)
    for i, tag in enumerate(tags):
        t[i, :len(tag)] = torch.LongTensor(tag)
    return s, t

valid_loader = DataLoader(validation_data,batch_size=32, shuffle=False,collate_fn=collate_validation)
valid_loader_rows = DataLoader(validation_data_row,batch_size=32,shuffle=False,collate_fn=collate_validation)

class BiLSTMModel(nn.Module):
    def __init__(self, input_size, hidden_size, num_layers, output_size,embedding_weights):
        super(BiLSTMModel, self).__init__()
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        self.embedding = nn.Embedding.from_pretrained(torch.tensor(embedding_weights), freeze=True)
        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, bidirectional=True)
        self.fc = nn.Linear(hidden_size * 2, hidden_size)
        self.fc2 = nn.Linear(hidden_size, output_size)
        self.Relu=nn.ReLU()

    def forward(self, x1 ,x2):
        x1 = x1.to(torch.long)
        x2 = x2.to(torch.long)
        embedded1 = self.embedding(x1)
        embedded2 = self.embedding(x2)
        embedded = torch.cat((embedded1, embedded2), dim=1)
        lstm_out, _ = self.lstm(embedded)
        output = self.fc(lstm_out)
        output=self.Relu(output)
        output_probs=self.fc2(output)
        return output_probs

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

input_size = 100
hidden_size = 64
num_layers = 2
output_size = 64

bilstm_model1 = BiLSTMModel(input_size,hidden_size, num_layers,output_size,embedding_matrix).to(device)
bilstm_model1.load_state_dict(torch.load('2019MT60763/2019MT60763/2019MT60763_BSMODEL_COL.pth'))

for param in bilstm_model1.parameters():
    param.data = param.data.double()

criterion = nn.CrossEntropyLoss(ignore_index=0)
optimizer = torch.optim.Adam(bilstm_model1.parameters(), lr=0.01)


column_prediction_val = []
row_prediction_val = []

def accuracy_col(predicte,l,revese_vocab):
    
    t2 = []
    for i in range(len(predicte)):
        l2 = predicte[i]
        for j in range(len(l2)):
            t2.append(l2[j])
    # print(t2)
    # print(l)
    p2=[]
    for i in range(len(t2)):
        e2 = t2[i]
        if(e2>=len(l[i])):
            p2.append(-1)
        elif(l[i][e2] in revese_vocab):
            p2.append(revese_vocab[l[i][e2]])
        else:
            p2.append(-1)
    #print(p2)
    return p2



with torch.no_grad():
    valid_true_labels = []
    valid_predicted_labels = []

    for inputs_questions,inputs_columns in tqdm(valid_loader,leave=False):
        inputs_questions = inputs_questions.to(device)
        inputs_columns = inputs_columns.to(device)
        
        outputs = bilstm_model1(inputs_questions,inputs_columns)
        temp=torch.mean(outputs, dim=1)
        temp=temp.cpu().numpy()
        output=np.argmax(temp,axis=1)
        valid_predicted_labels.append(output)
    column_prediction_val = accuracy_col(valid_predicted_labels,validation_data_Columns,reverse_building_vocablary)


input_size = 100
hidden_size = 32
num_layers = 2
output_size = 1
bilstm_model2 = BiLSTMModel(input_size,hidden_size,num_layers,output_size,embedding_matrix).to(device)
bilstm_model2.load_state_dict(torch.load('2019MT60763/2019MT60763/2019MT60763_BSMODEL_ROW.pth'))

for param in bilstm_model2.parameters():
    param.data = param.data.double()

criterion = nn.BCELoss()
optimizer = torch.optim.Adam(bilstm_model2.parameters(), lr=0.01)

def onehot_to_labels(text):
    p=[]
    for i in range(len(text)):
        if(text[i]==1):
            p.append(i)
    return p


def accuracy(predicted_labels,Rows_length):
    t2 = []
    for i in range(len(predicted_labels)):
        l2 = predicted_labels[i]
        for j in range(len(l2)):
            t2.append(l2[j])
    t3 = []
    t4 = []
    counter=0
    for i in range(len(Rows_length)):
        temp1=[]
        temp2=[]
        for p in range(Rows_length[i]):
            temp2.append(t2[counter+p])
        counter+=Rows_length[i]
        t3.append(onehot_to_labels(temp1))
        t4.append(onehot_to_labels(temp2))
    return t4

with torch.no_grad():
    valid=0
    valid_predicted_labels = []

    for inputs_questions,inputs_rows in tqdm(valid_loader_rows,leave=False):
        inputs_questions = inputs_questions.to(device)
        inputs_rows = inputs_rows.to(device)
        
        outputs = bilstm_model2(inputs_questions,inputs_rows)
        temp=torch.mean(outputs, dim=1)
        temp = temp.squeeze(dim=1)
        temp = temp.cpu().numpy()
        output = expit(temp)
        threshold = 0.2
        binary_vector = np.where(output > threshold, 1, 0)
        valid_predicted_labels.append(binary_vector)
    row_prediction_val = accuracy(valid_predicted_labels,validation_data_Rows_length)

train_prediction = []
for i in range(len(column_prediction_val)):
    train = {}
    p1=[]
    p1.append(column_prediction_val[i])
    p2=row_prediction_val[i]
    t=[]
    for j in range(len(p2)):
        t.append(p2[j])
    l=[]
    for j in range(len(p2)):
        a1=[]
        a1.append(p2[j])
        a1.append(column_prediction_val[i])
        l.append(a1)
    train["label_col"] = p1
    train["label_cell"] = l
    train["label_row"] = t
    train["qid"] = validation_Qid[i]
    train_prediction.append(train)

with open(sys.argv[2], "a") as json_file:
    length = len(train_prediction)
    for i in range(len(train_prediction)-1):
        json.dump(train_prediction[i], json_file)
        json_file.write('\n')
    json.dump(train_prediction[length-1], json_file)