# -*- coding: utf-8 -*-
"""train.py

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1sXFmbF22wZeuXiavOgkNqKuWp3QmkS9A
"""

import json
import math
import string
import re
import numpy as np
from random import sample
import torch
from torch.utils.data import Dataset, DataLoader
import copy
import torch.nn as nn
import torch.nn.functional as F
from tqdm import tqdm
from scipy.special import expit
import sys


np.random.seed(42)

def load_jsonl(file_path):
    data = []
    with open(file_path, 'r') as file:
        for line in file:
            data.append(json.loads(line))
    return data

train_jsonl_data = load_jsonl(sys.argv[1])

train_data_len = len(train_jsonl_data)

def list_to_store_questions_table(data):

    dict_to_store_questions_table = []
    for i in range(len(data)):
        dict = {}
        label_col = data[i]["label_col"]
        dict["Label_Col"] = label_col
        label_row = data[i]["label_row"]
        dict["Label_Row"] = label_row
        questions = data[i]["question"]
        dict["Question"] = questions
        dict["Columns"] = data[i]["table"]["cols"]
        dict["Row"] = data[i]["table"]["rows"]
        dict["qid"] = data[i]["qid"]
        dict_to_store_questions_table.append(dict)
    return dict_to_store_questions_table

def building_vocablary(text):
    c=2
    dict = {}
    dict["<unk>"]=0
    dict["<pad>"]=1
    for i in range(len(text)):
        L1 = text[i]
        Question = L1["Question"]
        Question_list = Question.split()
        for j in range(len(Question_list)):
            if Question_list[j] in dict:
                continue
            else:
                dict[Question_list[j]]=c
                c+=1
        Columns = L1["Columns"]
        for j in range(len(Columns)):
            if Columns[j] in dict:
                continue
            else:
                dict[Columns[j]]=c
                c+=1
        Row = L1["Row"]
        for j in range(len(Row)):
            E = Row[j]
            for k in range(len(E)):
                if E[k] in dict:
                    continue
                else:
                    dict[E[k]]=c
                    c+=1

    return dict

training_dict_to_store_questions_table = list_to_store_questions_table(train_jsonl_data)

vocab = building_vocablary(training_dict_to_store_questions_table)
vocab_size = len(vocab)

def reverse_building_vocablary(vocab):
    dict = {}
    for key in vocab:
        c = vocab[key]
        dict[c]=key
    return dict

reverse_building_vocablary = reverse_building_vocablary(vocab)

import gensim.downloader as api
glove_embeddings = api.load("glove-wiki-gigaword-100")


def finding_embedding_of_word(word,glove_embedding):
    if(word in glove_embedding):
        return glove_embedding[word]
    else:
        return np.random.rand(100)

def create_embedding_matrix(word_index, embeddings_index):
    embedding_matrix = np.zeros((len(word_index) + 1, 100))
    for word, i in word_index.items():
        word_list = word.split()
        if(len(word_list)>1):
            p = []
            for i in range(len(word_list)):
                p.append(finding_embedding_of_word(word_list[i],embeddings_index))
            t = np.mean(np.stack(p), axis=0)
            embedding_matrix[i] = t
        else:

            if(word in embeddings_index):
                embedding_matrix[i] = embeddings_index[word]
            else:
                embedding_matrix[i] = np.random.rand(100)
    return embedding_matrix

embedding_matrix = create_embedding_matrix(vocab, glove_embeddings)

def finding_column_number(p,text):
    for i in range(len(p)):
        if(p[i]==text):
            return i
    return 0

def replacing_with_id(dict_to_store_questions_table,vocab):

    list_to_store_questions_table_id = []
    labels_col = []
    labels_row = []
    for i in range(len(dict_to_store_questions_table)):
        dict = {}
        L = dict_to_store_questions_table[i]
        labels_col.append(finding_column_number(L["Columns"],L["Label_Col"][0]))
        labels_row.append(L["Label_Row"])
        Question = L["Question"]
        Question_list = Question.split()
        p = []
        for j in range(len(Question_list)):
            if(Question_list[j] in vocab):
                p.append(vocab[Question_list[j]])
            else:
                p.append(vocab["<unk>"])
        dict["Question"] = p
        t = []
        Columns = L["Columns"]
        for j in range(len(Columns)):
            if(Columns[j] in vocab):
                t.append(vocab[Columns[j]])
            else:
                t.append(vocab["<unk>"])
        dict["Columns"] = t
        temp = []
        Rows = L["Row"]
        for j in range(len(Rows)):
            E = Rows[j]
            r=[]
            for k in range(len(E)):
                if(E[k] in vocab):
                    r.append(vocab[E[k]])
                else:
                    r.append(vocab["<unk>"])
            temp.append(r)
        dict["Rows"]=temp
        dict["qid"] = L["qid"]
        list_to_store_questions_table_id.append(dict)
    return list_to_store_questions_table_id,labels_col,labels_row

traning_List_for_storing_question_and_column_with_id,training_targets_col,training_targets_row = replacing_with_id(training_dict_to_store_questions_table,vocab)

def preparing_input_data(List_for_storing_question_and_column_with_id):
    Questions = []
    Columns = []
    Rows = []
    Rows_length = []
    Qid = []
    for i in range(len(List_for_storing_question_and_column_with_id)):
        L = List_for_storing_question_and_column_with_id[i]
        Questions.append(L["Question"])
        Columns.append(L["Columns"])
        Rows.append(L["Rows"])
        Rows_length.append(len(L["Rows"]))
        Qid.append(L["qid"])
    return Questions,Columns,Rows,Rows_length,Qid

training_data_Questions,training_data_Columns,training_data_Rows,training_data_Rows_length,training_Qid = preparing_input_data(traning_List_for_storing_question_and_column_with_id)

t = copy.copy(training_data_Questions)
training_data = []

for i in range(len(training_data_Questions)):
    training_data.append((training_data_Questions[i],training_data_Columns[i],training_targets_col[i]))

training_data_row = []

def check_whether_sample_exits_or_not(l,p):
    for i in range(len(l)):
        if(l[i]==p):
            return 1
    return 0

for i in range(len(training_data_Rows)):
    for j in range(len(training_data_Rows[i])):
         if(check_whether_sample_exits_or_not(training_targets_row[i],j)):
            training_data_row.append((t[i],training_data_Rows[i][j],1))
         else:
            training_data_row.append((t[i],training_data_Rows[i][j],0))

with open('2019MT60763/vocab.json','w') as f:
    json.dump(vocab,f)
with open('2019MT60763/reverse_vocab.json','w') as f:
    json.dump(reverse_building_vocablary,f)


def collate(batch):
    sentences = [x[0] for x in batch]
    max_len = max(len(s) for s in sentences)
    s = torch.zeros((len(sentences), max_len), dtype=torch.long)
    for i, sentence in enumerate(sentences):
        s[i, :len(sentence)] = torch.LongTensor(sentence)

    tags = [x[1] for x in batch]
    max_tag_len = max(len(t) for t in tags)
    t = torch.zeros((len(tags), max_tag_len), dtype=torch.long)
    for i, tag in enumerate(tags):
        t[i, :len(tag)] = torch.LongTensor(tag)
    l = [x[2] for x in batch]
    l = torch.LongTensor(l)
    return s, t, l


train_loader = DataLoader(training_data,batch_size=32, shuffle=False,collate_fn=collate)
train_loader_rows = DataLoader(training_data_row,batch_size=32,shuffle=False,collate_fn=collate)

class BiLSTMModel(nn.Module):
    def __init__(self, input_size, hidden_size, num_layers, output_size,embedding_weights):
        super(BiLSTMModel, self).__init__()
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        self.embedding = nn.Embedding.from_pretrained(torch.tensor(embedding_weights), freeze=True)
        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, bidirectional=True)
        self.fc = nn.Linear(hidden_size * 2, hidden_size)
        self.fc2 = nn.Linear(hidden_size, output_size)
        self.Relu=nn.ReLU()

    def forward(self, x1 ,x2):
        x1 = x1.to(torch.long)
        x2 = x2.to(torch.long)
        embedded1 = self.embedding(x1)
        embedded2 = self.embedding(x2)
        embedded = torch.cat((embedded1, embedded2), dim=1)
        lstm_out, _ = self.lstm(embedded)
        output = self.fc(lstm_out)
        output=self.Relu(output)
        output_probs=self.fc2(output)
        return output_probs

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

input_size = 100
hidden_size = 64
num_layers = 2
output_size = 64
bilstm_model1 = BiLSTMModel(input_size,hidden_size, num_layers,output_size,embedding_matrix).to(device)

for param in bilstm_model1.parameters():
    param.data = param.data.double()

criterion = nn.CrossEntropyLoss(ignore_index=0)
optimizer = torch.optim.Adam(bilstm_model1.parameters(), lr=0.01)

for epoch in range(100):
    running_loss = 0
    bilstm_model1.train()
    for inputs_questions,inputs_columns, labels in tqdm(train_loader,leave=False):
        bilstm_model1.zero_grad()
        inputs_questions = inputs_questions.to(device)
        inputs_columns = inputs_columns.to(device)
        labels = labels.to(device)
        outputs = bilstm_model1(inputs_questions,inputs_columns)
        temp=torch.mean(outputs, dim=1)
        loss = criterion(temp, labels)
        loss.backward()
        optimizer.step()
        running_loss += loss.item()


torch.save(bilstm_model1.state_dict(), '2019MT60763/2019MT60763_BSMODEL_COL.pth')
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

input_size = 100
hidden_size = 32
num_layers = 2
output_size = 1
bilstm_model2 = BiLSTMModel(input_size,hidden_size,num_layers,output_size,embedding_matrix).to(device)

for param in bilstm_model2.parameters():
    param.data = param.data.double()

criterion = nn.BCELoss()
optimizer = torch.optim.Adam(bilstm_model2.parameters(), lr=0.01)

for epoch in range(30):
    running_loss = 0
    bilstm_model2.train()
    for inputs_questions,inputs_rows,labels in tqdm(train_loader_rows,leave=False):
            bilstm_model2.zero_grad()

            inputs_questions = inputs_questions.to(device)
            inputs_rows = inputs_rows.to(device)
            labels = labels.to(device)
            outputs = bilstm_model2(inputs_questions,inputs_rows)
            temp = torch.mean(outputs, dim=1)
            temp = temp.squeeze(dim=1)
            labels = labels.to(torch.float64)
            loss = criterion(torch.sigmoid(temp), labels)
            loss.backward()
            optimizer.step()
            running_loss += loss.item()


torch.save(bilstm_model2.state_dict(), '2019MT60763/2019MT60763_BSMODEL_ROW.pth')
